# Environment settings
env:
  ports: 4 # [4, 5, 6]
  bays: 10 # [10, 20,]
  decks: 2
  cargo_classes: 6
  weight_classes: 3
  customer_classes: 2
  capacity: [50] # [50, 500]
  TEU: 1000  # 1000, 20000
  LCG_target: 0.95
  VCG_target: 1.05
  stability_difference: 0.10
  CI_target: 1.25
  hatch_overstowage_costs: 0.333333
  hatch_overstowage_mask: False
  long_crane_costs: 0.5
  episode_order: "standard" # greedy_revenue, priority_then_greedy, max_distance_then_priority, standard
  normalize_obs: True

  # Generator
  seed: 42
  utilization_rate_initial_demand: 1.1
  spot_percentage: 0.3
  iid_demand: True
  cv_demand: 0.5
  demand_uncertainty: True
  generalization: False

# Model settings
model:
  lr_end_factor: 0.5
  batch_size: 128
  init_dim: 8
  embed_dim: 128
  hidden_dim: 128
  num_encoder_layers: 3
  num_decoder_layers: 2
  num_heads: 8
  dropout_rate: 0.09866208728742974
  normalization: "layer" # layer, batch
  out_bias_pointer_attn: False
  temperature: 1.0
  critic_temperature: 1.0
  tanh_clipping: 0
  tanh_squashing: False
  scale_max: 0.75
  train_decode_type: "continuous_sampling"
  val_decode_type: "continuous_projection"
  test_decode_type: "continuous_projection"
  phase: "train" # train, test
  encoder_type: "attention" # mlp, attention
  decoder_type: "attention" # mlp, attention
  demand_aggregation: "full" # sum, self_attn, full
  logger: "wandb" #"wandb" # None, wandb

# PPO parameters
algorithm:
  type: "sac" # ppo, ddpg, sac
  clip_range: 0.2
  ppo_epochs: 5 # 2, 3, 5, 7, 10
  mini_batch_size: 0.5
  normalize_adv: False
  normalize_return: False
  max_grad_norm: 0.5
  tau: 0.005
  kl_threshold: None #0.01 - 0.1
  kl_penalty_lambda: None # 1.0
  gamma: 0.99
  gae_lambda: 0.95
  vf_lambda: 0.5
  entropy_lambda: 0.01
  feasibility_lambda: 0.14387397215952227
  demand_lambda: 1.0
  stability_lambda: 1.0
  projection_lambda: 0.01
  adaptive_feasibility_lambda: False
  n_step: 72

# AM PPO parameters
training:
  lr: 0.00012077516375897934
  projection_type: "weighted_scaling_policy_clipping" #"linear_violation", "weighted_scaling_policy_clipping", "None"
  #todo: not implemented logprob adaptations - convex_program, worst_case_violation, linear_violation, linear_program
  projection_kwargs:
    alpha: 0.001
    iters: 5
    project_per_port: False
  optimizer: "Adam"   #AdamW, Adam
  validation_freq: 0.2
  validation_patience: 2
  train_data_size: 72_000_000  #5_000_000 # 3_000_000
  val_data_size: 5000
  test_data_size: 5000

testing:
  timestamp: "20250123_171145"
  num_episodes: 30